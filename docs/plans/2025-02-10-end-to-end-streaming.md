# End-to-End Streaming Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Stream nlm-proxy SSE chunks directly to the user via the M365 Agents SDK `StreamingResponse`, replacing the current buffer-everything-then-send approach.

**Architecture:** Add an `AsyncGenerator`-based `query_stream()` method to `NLMClient` that yields individual chunks. The bot handler iterates this generator, piping reasoning tokens, a separator, and content tokens through `StreamingResponse.queue_text_chunk()`. Notebook name is shown as a status update via `queue_informative_update()`.

**Tech Stack:** AsyncOpenAI SDK (streaming), M365 Agents SDK `StreamingResponse`, Python `AsyncGenerator`, dataclasses

---

## Current vs Target Flow

```
BEFORE: User → typing indicator → await query() [blocks 5-30s] → send_activity(full_text)

AFTER:  User → StreamingResponse(context)
             → queue_informative_update("Searching HR Docs...")
             → queue_text_chunk(reasoning_delta)         [streamed]
             → queue_text_chunk("\n\n---\n\n")            [separator]
             → queue_text_chunk(content_delta)            [streamed]
             → queue_text_chunk("\n---\n*Source: HR Docs*") [attribution]
             → await end_stream()
```

## SDK Reference (verified in .venv)

Import: `from microsoft_agents.hosting.aiohttp.app.streaming.streaming_response import StreamingResponse`

| Method | Signature | Notes |
|--------|-----------|-------|
| constructor | `StreamingResponse(context: TurnContext)` | Auto-detects channel (Teams=1s interval, DirectLine=0.5s, stream mode=0.1s) |
| `queue_informative_update` | `(text: str) -> None` | No-ops on non-streaming channels |
| `queue_text_chunk` | `(text: str) -> None` | Accumulates internally, sends at intervals |
| `end_stream` | `async () -> None` | **Must await.** Sends final message activity |
| `set_generated_by_ai_label` | `(bool) -> None` | Adds "Generated by AI" label |

Non-streaming channels: `queue_text_chunk` accumulates text, `end_stream()` sends it as one message. Same code path works everywhere.

---

### Task 1: Add `NLMChunk` Model

**Files:**
- Modify: `src/knowledge_finder_bot/nlm/models.py`
- Modify: `src/knowledge_finder_bot/nlm/__init__.py`

**Step 1: Write the failing test**

Create `tests/test_nlm_streaming.py`:

```python
"""Tests for NLMChunk model."""

from knowledge_finder_bot.nlm.models import NLMChunk


def test_nlm_chunk_content():
    """NLMChunk holds content chunk data."""
    chunk = NLMChunk(chunk_type="content", text="Hello")
    assert chunk.chunk_type == "content"
    assert chunk.text == "Hello"
    assert chunk.model is None


def test_nlm_chunk_meta():
    """NLMChunk holds meta chunk data."""
    chunk = NLMChunk(chunk_type="meta", model="hr-notebook", conversation_id="abc")
    assert chunk.chunk_type == "meta"
    assert chunk.model == "hr-notebook"
    assert chunk.conversation_id == "abc"
    assert chunk.text is None
```

**Step 2: Run test to verify it fails**

Run: `uv run pytest tests/test_nlm_streaming.py::test_nlm_chunk_content tests/test_nlm_streaming.py::test_nlm_chunk_meta -v`
Expected: FAIL with `ImportError: cannot import name 'NLMChunk'`

**Step 3: Write minimal implementation**

Add to `src/knowledge_finder_bot/nlm/models.py` after existing code:

```python
from dataclasses import dataclass
from typing import Literal


@dataclass(slots=True)
class NLMChunk:
    """A single chunk from the nlm-proxy stream."""

    chunk_type: Literal["reasoning", "content", "meta"]
    text: str | None = None
    model: str | None = None
    conversation_id: str | None = None
    finish_reason: str | None = None
```

Update `src/knowledge_finder_bot/nlm/__init__.py`:

```python
"""nlm-proxy client module."""

from knowledge_finder_bot.nlm.client import NLMClient
from knowledge_finder_bot.nlm.models import NLMChunk, NLMResponse

__all__ = ["NLMClient", "NLMChunk", "NLMResponse"]
```

**Step 4: Run test to verify it passes**

Run: `uv run pytest tests/test_nlm_streaming.py -v`
Expected: 2 PASSED

**Step 5: Run full existing suite for regressions**

Run: `uv run pytest tests/ -v`
Expected: All existing tests still pass

**Step 6: Commit**

```bash
git add src/knowledge_finder_bot/nlm/models.py src/knowledge_finder_bot/nlm/__init__.py tests/test_nlm_streaming.py
git commit -m "feat(nlm): add NLMChunk dataclass for streaming chunks"
```

---

### Task 2: Add `query_stream()` AsyncGenerator to NLMClient

**Files:**
- Modify: `src/knowledge_finder_bot/nlm/client.py`
- Modify: `tests/test_nlm_streaming.py`

**Step 1: Write the failing tests**

Append to `tests/test_nlm_streaming.py`. Reuse the `make_chunk` helper pattern from `tests/test_nlm_client.py:108-122`:

```python
from unittest.mock import AsyncMock, MagicMock, patch

import pytest

from knowledge_finder_bot.nlm.client import NLMClient


@pytest.fixture
def nlm_settings(mock_env_vars):
    """Settings with nlm-proxy configured."""
    from knowledge_finder_bot.config import Settings

    with patch.dict("os.environ", {
        **mock_env_vars,
        "NLM_PROXY_URL": "http://localhost:8000/v1",
        "NLM_PROXY_API_KEY": "test-key",
    }):
        return Settings()


def make_chunk(content=None, reasoning=None, finish_reason=None,
               model=None, system_fingerprint=None):
    """Build a mock SSE chunk matching OpenAI streaming format."""
    delta = MagicMock()
    delta.content = content
    delta.reasoning_content = reasoning

    choice = MagicMock()
    choice.delta = delta
    choice.finish_reason = finish_reason

    chunk = MagicMock()
    chunk.choices = [choice]
    chunk.model = model
    chunk.system_fingerprint = system_fingerprint
    return chunk


async def _collect_chunks(gen):
    """Collect all chunks from an async generator into a list."""
    result = []
    async for chunk in gen:
        result.append(chunk)
    return result


@pytest.mark.asyncio
async def test_query_stream_yields_reasoning_chunks(nlm_settings):
    """Reasoning content arrives as chunk_type='reasoning'."""
    client = NLMClient(nlm_settings)

    chunks = [
        make_chunk(reasoning="Thinking about ", model="kf"),
        make_chunk(reasoning="the answer"),
        make_chunk(content="Result", finish_reason="stop"),
    ]

    async def mock_stream():
        for c in chunks:
            yield c

    client._client.chat.completions.create = AsyncMock(return_value=mock_stream())

    result = await _collect_chunks(client.query_stream(
        user_message="Test",
        allowed_notebooks=["nb-1"],
    ))

    reasoning_chunks = [c for c in result if c.chunk_type == "reasoning"]
    assert len(reasoning_chunks) == 2
    assert reasoning_chunks[0].text == "Thinking about "
    assert reasoning_chunks[1].text == "the answer"


@pytest.mark.asyncio
async def test_query_stream_yields_content_chunks(nlm_settings):
    """Answer content arrives as chunk_type='content'."""
    client = NLMClient(nlm_settings)

    chunks = [
        make_chunk(content="Hello ", model="kf"),
        make_chunk(content="world!", finish_reason="stop"),
    ]

    async def mock_stream():
        for c in chunks:
            yield c

    client._client.chat.completions.create = AsyncMock(return_value=mock_stream())

    result = await _collect_chunks(client.query_stream(
        user_message="Test",
        allowed_notebooks=["nb-1"],
    ))

    content_chunks = [c for c in result if c.chunk_type == "content"]
    assert len(content_chunks) == 2
    assert content_chunks[0].text == "Hello "
    assert content_chunks[1].text == "world!"


@pytest.mark.asyncio
async def test_query_stream_yields_model_meta(nlm_settings):
    """First chunk with model field yields a meta chunk."""
    client = NLMClient(nlm_settings)

    chunks = [
        make_chunk(reasoning="think", model="hr-notebook"),
        make_chunk(reasoning="more", model="hr-notebook"),  # second time - no duplicate meta
        make_chunk(finish_reason="stop"),
    ]

    async def mock_stream():
        for c in chunks:
            yield c

    client._client.chat.completions.create = AsyncMock(return_value=mock_stream())

    result = await _collect_chunks(client.query_stream(
        user_message="Test",
        allowed_notebooks=["nb-1"],
    ))

    model_metas = [c for c in result if c.chunk_type == "meta" and c.model is not None]
    assert len(model_metas) == 1
    assert model_metas[0].model == "hr-notebook"


@pytest.mark.asyncio
async def test_query_stream_yields_conversation_id(nlm_settings):
    """system_fingerprint='conv_xxx' yields meta chunk with conversation_id."""
    client = NLMClient(nlm_settings)

    chunks = [
        make_chunk(content="Hi", model="kf", system_fingerprint="conv_abc123"),
        make_chunk(finish_reason="stop"),
    ]

    async def mock_stream():
        for c in chunks:
            yield c

    client._client.chat.completions.create = AsyncMock(return_value=mock_stream())

    result = await _collect_chunks(client.query_stream(
        user_message="Test",
        allowed_notebooks=["nb-1"],
    ))

    conv_metas = [c for c in result if c.chunk_type == "meta" and c.conversation_id is not None]
    assert len(conv_metas) >= 1
    assert conv_metas[0].conversation_id == "abc123"


@pytest.mark.asyncio
async def test_query_stream_yields_finish_reason(nlm_settings):
    """Last chunk yields meta with finish_reason."""
    client = NLMClient(nlm_settings)

    chunks = [
        make_chunk(content="Done", model="kf"),
        make_chunk(finish_reason="stop"),
    ]

    async def mock_stream():
        for c in chunks:
            yield c

    client._client.chat.completions.create = AsyncMock(return_value=mock_stream())

    result = await _collect_chunks(client.query_stream(
        user_message="Test",
        allowed_notebooks=["nb-1"],
    ))

    finish_metas = [c for c in result if c.chunk_type == "meta" and c.finish_reason is not None]
    assert len(finish_metas) == 1
    assert finish_metas[0].finish_reason == "stop"


@pytest.mark.asyncio
async def test_query_stream_propagates_error(nlm_settings):
    """Exception during streaming propagates to caller."""
    client = NLMClient(nlm_settings)
    client._client.chat.completions.create = AsyncMock(
        side_effect=Exception("Connection refused")
    )

    with pytest.raises(Exception, match="Connection refused"):
        async for _ in client.query_stream(
            user_message="Test",
            allowed_notebooks=["nb-1"],
        ):
            pass


@pytest.mark.asyncio
async def test_query_stream_passes_metadata(nlm_settings):
    """allowed_notebooks and conversation_id passed in extra_body."""
    client = NLMClient(nlm_settings)

    async def mock_stream():
        yield make_chunk(content="ok", model="kf", finish_reason="stop")

    client._client.chat.completions.create = AsyncMock(return_value=mock_stream())

    async for _ in client.query_stream(
        user_message="Test",
        allowed_notebooks=["nb-1", "nb-2"],
        conversation_id="existing-conv",
    ):
        pass

    call_kwargs = client._client.chat.completions.create.call_args.kwargs
    assert call_kwargs["extra_body"]["metadata"]["allowed_notebooks"] == ["nb-1", "nb-2"]
    assert call_kwargs["extra_body"]["conversation_id"] == "existing-conv"
    assert call_kwargs["stream"] is True
```

**Step 2: Run tests to verify they fail**

Run: `uv run pytest tests/test_nlm_streaming.py -v`
Expected: FAIL with `AttributeError: 'NLMClient' object has no attribute 'query_stream'`

**Step 3: Write minimal implementation**

Add to `src/knowledge_finder_bot/nlm/client.py` — new import at top and new method on `NLMClient`:

Add import at line 1 area:
```python
from collections.abc import AsyncGenerator
```

Add import of NLMChunk alongside existing NLMResponse import at line 7:
```python
from knowledge_finder_bot.nlm.models import NLMChunk, NLMResponse
```

Add new method after `query()` (after line 66), before `_query_streaming()`:

```python
    async def query_stream(
        self,
        user_message: str,
        allowed_notebooks: list[str],
        conversation_id: str | None = None,
    ) -> AsyncGenerator[NLMChunk, None]:
        """Stream nlm-proxy response as individual chunks.

        Yields NLMChunk objects as they arrive from the SSE stream.
        The caller is responsible for accumulating text.
        """
        extra_body: dict = {"metadata": {"allowed_notebooks": allowed_notebooks}}
        if conversation_id:
            extra_body["conversation_id"] = conversation_id

        logger.info(
            "nlm_stream_start",
            model=self._model,
            notebook_count=len(allowed_notebooks),
            has_conversation_id=conversation_id is not None,
        )

        stream = await self._client.chat.completions.create(
            model=self._model,
            messages=[{"role": "user", "content": user_message}],
            stream=True,
            extra_body=extra_body,
        )

        model_emitted = False

        async for chunk in stream:
            chunk_model = chunk.model if chunk.model else None
            sys_fp = chunk.system_fingerprint
            parsed_conv_id = _parse_conversation_id(sys_fp) if sys_fp else None

            if (chunk_model and not model_emitted) or parsed_conv_id:
                yield NLMChunk(
                    chunk_type="meta",
                    model=chunk_model if not model_emitted else None,
                    conversation_id=parsed_conv_id,
                )
                if chunk_model:
                    model_emitted = True

            for choice in chunk.choices:
                delta = choice.delta

                reasoning = getattr(delta, "reasoning_content", None)
                if reasoning:
                    yield NLMChunk(chunk_type="reasoning", text=reasoning)

                if delta.content:
                    yield NLMChunk(chunk_type="content", text=delta.content)

                if choice.finish_reason:
                    yield NLMChunk(
                        chunk_type="meta",
                        finish_reason=choice.finish_reason,
                    )

        logger.info("nlm_stream_complete", model=self._model)
```

**Step 4: Run tests to verify they pass**

Run: `uv run pytest tests/test_nlm_streaming.py -v`
Expected: 9 PASSED (2 model tests + 7 streaming tests)

**Step 5: Run full suite for regressions**

Run: `uv run pytest tests/ -v`
Expected: All existing tests still pass

**Step 6: Commit**

```bash
git add src/knowledge_finder_bot/nlm/client.py tests/test_nlm_streaming.py
git commit -m "feat(nlm): add query_stream() async generator for real-time chunk yielding"
```

---

### Task 3: Add `format_source_attribution()` to Formatter

**Files:**
- Modify: `src/knowledge_finder_bot/nlm/formatter.py`
- Modify: `tests/test_nlm_formatter.py`

**Step 1: Write the failing tests**

Append to `tests/test_nlm_formatter.py`:

```python
from knowledge_finder_bot.nlm.formatter import format_source_attribution


def test_format_source_attribution_known_notebook():
    """Returns formatted source line for known notebook ID."""
    acl_service = MagicMock()
    acl_service.get_notebook_name.return_value = "HR Docs"

    result = format_source_attribution("hr-notebook", acl_service)
    assert result == "\n---\n*Source: HR Docs*"


def test_format_source_attribution_unknown_notebook():
    """Returns None when notebook ID not recognized."""
    acl_service = MagicMock()
    acl_service.get_notebook_name.return_value = None

    result = format_source_attribution("unknown-id", acl_service)
    assert result is None


def test_format_source_attribution_no_acl():
    """Returns None when acl_service is None."""
    result = format_source_attribution("hr-notebook", acl_service=None)
    assert result is None


def test_format_source_attribution_no_notebook_id():
    """Returns None when notebook_id is None."""
    acl_service = MagicMock()
    result = format_source_attribution(None, acl_service)
    assert result is None
```

**Step 2: Run tests to verify they fail**

Run: `uv run pytest tests/test_nlm_formatter.py::test_format_source_attribution_known_notebook -v`
Expected: FAIL with `ImportError: cannot import name 'format_source_attribution'`

**Step 3: Write minimal implementation**

Add to `src/knowledge_finder_bot/nlm/formatter.py` after existing `format_response()`:

```python
def format_source_attribution(
    notebook_id: str | None,
    acl_service: ACLService | None = None,
) -> str | None:
    """Return source attribution line for a notebook, or None."""
    if notebook_id and acl_service:
        notebook_name = acl_service.get_notebook_name(notebook_id)
        if notebook_name:
            return f"\n---\n*Source: {notebook_name}*"
    return None
```

**Step 4: Run tests to verify they pass**

Run: `uv run pytest tests/test_nlm_formatter.py -v`
Expected: 9 PASSED (5 existing + 4 new)

**Step 5: Commit**

```bash
git add src/knowledge_finder_bot/nlm/formatter.py tests/test_nlm_formatter.py
git commit -m "feat(nlm): add format_source_attribution() for streaming source lines"
```

---

### Task 4: Rewrite Bot Handler for Streaming

**Files:**
- Modify: `src/knowledge_finder_bot/bot/bot.py:1-30` (imports) and `bot.py:204-224` (handler logic)
- Modify: `tests/test_bot_nlm.py` (rewrite fixtures and tests)

**Step 1: Write the failing tests**

Rewrite `tests/test_bot_nlm.py` completely. The key change: mock `query_stream` (async generator) instead of `query` (async return), and patch `StreamingResponse`:

```python
"""Tests for bot handler with nlm-proxy streaming integration."""

import pytest
from unittest.mock import AsyncMock, MagicMock, patch

from knowledge_finder_bot.auth.graph_client import UserInfo
from knowledge_finder_bot.bot import create_agent_app
from knowledge_finder_bot.config import Settings
from knowledge_finder_bot.nlm.models import NLMChunk, NLMResponse
from knowledge_finder_bot.nlm.session import SessionStore


def create_mock_context(
    activity_type: str,
    text: str = None,
    aad_object_id: str = None,
):
    """Create a mock turn context for bot tests."""
    context = MagicMock()
    context.send_activity = AsyncMock()
    context.remove_recipient_mention.return_value = text

    context.activity = MagicMock()
    context.activity.type = activity_type
    context.activity.text = text
    context.activity.from_property = MagicMock()
    context.activity.from_property.name = "Test User"
    context.activity.from_property.aad_object_id = aad_object_id
    context.activity.recipient = MagicMock()
    context.activity.recipient.id = "bot-id"
    context.activity.members_added = None

    return context


def _make_default_chunks():
    """Standard chunk sequence: meta(model) -> reasoning -> content -> meta(conv_id) -> meta(finish)."""
    return [
        NLMChunk(chunk_type="meta", model="hr-notebook"),
        NLMChunk(chunk_type="reasoning", text="Looking in HR docs"),
        NLMChunk(chunk_type="content", text="The leave policy "),
        NLMChunk(chunk_type="content", text="allows 20 days per year."),
        NLMChunk(chunk_type="meta", conversation_id="conv-123"),
        NLMChunk(chunk_type="meta", finish_reason="stop"),
    ]


@pytest.fixture
def mock_nlm_client():
    """Mock NLMClient with query_stream returning async generator."""
    client = MagicMock()

    async def _default_stream(**kwargs):
        for chunk in _make_default_chunks():
            yield chunk

    client.query_stream = MagicMock(side_effect=lambda **kw: _default_stream(**kw))
    return client


@pytest.fixture
def mock_streaming_response():
    """Mock StreamingResponse capturing all method calls."""
    sr = MagicMock()
    sr.queue_informative_update = MagicMock()
    sr.queue_text_chunk = MagicMock()
    sr.end_stream = AsyncMock()
    sr.set_generated_by_ai_label = MagicMock()
    return sr


@pytest.fixture
def session_store():
    """Real SessionStore for testing multi-turn."""
    return SessionStore(ttl=300, maxsize=100)


@pytest.fixture
def nlm_app(settings, acl_config_path, mock_graph_client, mock_nlm_client, session_store):
    """Agent app with ACL + nlm-proxy streaming integration."""
    from knowledge_finder_bot.acl.service import ACLService

    acl_service = ACLService(acl_config_path)
    return create_agent_app(
        settings=settings,
        graph_client=mock_graph_client,
        acl_service=acl_service,
        nlm_client=mock_nlm_client,
        session_store=session_store,
    )


@pytest.mark.asyncio
async def test_streaming_query_stream_called(nlm_app, mock_nlm_client, mock_streaming_response):
    """query_stream is called with correct args when nlm_client configured."""
    context = create_mock_context(
        activity_type="message",
        text="What is the leave policy?",
        aad_object_id="test-aad-id",
    )

    with patch(
        "knowledge_finder_bot.bot.bot.StreamingResponse",
        return_value=mock_streaming_response,
    ):
        await nlm_app.on_turn(context)

    mock_nlm_client.query_stream.assert_called_once()
    call_kwargs = mock_nlm_client.query_stream.call_args.kwargs
    assert call_kwargs["user_message"] == "What is the leave policy?"
    assert "hr-notebook" in call_kwargs["allowed_notebooks"]


@pytest.mark.asyncio
async def test_streaming_content_sent_via_queue_text_chunk(nlm_app, mock_nlm_client, mock_streaming_response):
    """Content chunks are piped to StreamingResponse.queue_text_chunk."""
    context = create_mock_context(
        activity_type="message",
        text="Hello",
        aad_object_id="test-aad-id",
    )

    with patch(
        "knowledge_finder_bot.bot.bot.StreamingResponse",
        return_value=mock_streaming_response,
    ):
        await nlm_app.on_turn(context)

    text_calls = [
        call[0][0] for call in mock_streaming_response.queue_text_chunk.call_args_list
    ]
    # Should contain reasoning, separator, content chunks, and source attribution
    combined = "".join(text_calls)
    assert "Looking in HR docs" in combined  # reasoning
    assert "---" in combined  # separator
    assert "leave policy" in combined  # content
    assert "allows 20 days" in combined  # content


@pytest.mark.asyncio
async def test_streaming_informative_update_with_notebook_name(nlm_app, mock_nlm_client, mock_streaming_response):
    """queue_informative_update called with notebook name."""
    context = create_mock_context(
        activity_type="message",
        text="Hello",
        aad_object_id="test-aad-id",
    )

    with patch(
        "knowledge_finder_bot.bot.bot.StreamingResponse",
        return_value=mock_streaming_response,
    ):
        await nlm_app.on_turn(context)

    mock_streaming_response.queue_informative_update.assert_called_once()
    info_text = mock_streaming_response.queue_informative_update.call_args[0][0]
    assert "HR Docs" in info_text


@pytest.mark.asyncio
async def test_streaming_end_stream_called(nlm_app, mock_nlm_client, mock_streaming_response):
    """end_stream() is awaited after all chunks processed."""
    context = create_mock_context(
        activity_type="message",
        text="Hello",
        aad_object_id="test-aad-id",
    )

    with patch(
        "knowledge_finder_bot.bot.bot.StreamingResponse",
        return_value=mock_streaming_response,
    ):
        await nlm_app.on_turn(context)

    mock_streaming_response.end_stream.assert_awaited_once()


@pytest.mark.asyncio
async def test_streaming_source_attribution_appended(nlm_app, mock_nlm_client, mock_streaming_response):
    """Source attribution is the last text chunk before end_stream."""
    context = create_mock_context(
        activity_type="message",
        text="Hello",
        aad_object_id="test-aad-id",
    )

    with patch(
        "knowledge_finder_bot.bot.bot.StreamingResponse",
        return_value=mock_streaming_response,
    ):
        await nlm_app.on_turn(context)

    text_calls = [
        call[0][0] for call in mock_streaming_response.queue_text_chunk.call_args_list
    ]
    last_text = text_calls[-1]
    assert "*Source: HR Docs*" in last_text


@pytest.mark.asyncio
async def test_streaming_conversation_id_stored(nlm_app, mock_nlm_client, session_store, mock_streaming_response):
    """conversation_id from stream is stored in session for next turn."""
    context = create_mock_context(
        activity_type="message",
        text="First question",
        aad_object_id="test-aad-id",
    )

    with patch(
        "knowledge_finder_bot.bot.bot.StreamingResponse",
        return_value=mock_streaming_response,
    ):
        await nlm_app.on_turn(context)

    assert session_store.get("test-aad-id") == "conv-123"


@pytest.mark.asyncio
async def test_streaming_conversation_id_reused(nlm_app, mock_nlm_client, session_store, mock_streaming_response):
    """Stored conversation_id is passed to subsequent query_stream calls."""
    session_store.set("test-aad-id", "existing-conv")

    context = create_mock_context(
        activity_type="message",
        text="Follow-up question",
        aad_object_id="test-aad-id",
    )

    with patch(
        "knowledge_finder_bot.bot.bot.StreamingResponse",
        return_value=mock_streaming_response,
    ):
        await nlm_app.on_turn(context)

    call_kwargs = mock_nlm_client.query_stream.call_args.kwargs
    assert call_kwargs["conversation_id"] == "existing-conv"


@pytest.mark.asyncio
async def test_streaming_error_sends_error_message(nlm_app, mock_nlm_client, mock_streaming_response):
    """Exception during streaming sends error via end_stream."""
    async def _error_stream(**kwargs):
        yield NLMChunk(chunk_type="meta", model="hr-notebook")
        raise Exception("Stream broken")

    mock_nlm_client.query_stream = MagicMock(side_effect=lambda **kw: _error_stream(**kw))

    context = create_mock_context(
        activity_type="message",
        text="Hello",
        aad_object_id="test-aad-id",
    )

    with patch(
        "knowledge_finder_bot.bot.bot.StreamingResponse",
        return_value=mock_streaming_response,
    ):
        await nlm_app.on_turn(context)

    # Error message should be sent via queue_text_chunk + end_stream
    text_calls = [
        call[0][0] for call in mock_streaming_response.queue_text_chunk.call_args_list
    ]
    error_found = any("error" in t.lower() for t in text_calls)
    assert error_found, f"Error message not found in: {text_calls}"
    mock_streaming_response.end_stream.assert_awaited()


@pytest.mark.asyncio
async def test_fallback_to_echo_when_nlm_client_none(settings, acl_config_path, mock_graph_client):
    """Falls back to echo when nlm_client is None (unchanged behavior)."""
    from knowledge_finder_bot.acl.service import ACLService

    acl_service = ACLService(acl_config_path)
    app = create_agent_app(
        settings=settings,
        graph_client=mock_graph_client,
        acl_service=acl_service,
        nlm_client=None,
    )

    context = create_mock_context(
        activity_type="message",
        text="Hello!",
        aad_object_id="test-aad-id",
    )
    await app.on_turn(context)

    calls = context.send_activity.call_args_list
    echo_found = any(
        isinstance(c[0][0], str) and "Allowed notebooks" in c[0][0]
        for c in calls
    )
    assert echo_found, f"Echo fallback not found in: {calls}"
```

**Step 2: Run tests to verify they fail**

Run: `uv run pytest tests/test_bot_nlm.py -v`
Expected: FAIL — tests will fail because bot.py still calls `nlm_client.query()` not `query_stream()`, and `StreamingResponse` is not imported yet.

**Step 3: Write the implementation**

Modify `src/knowledge_finder_bot/bot/bot.py`:

**3a. Update imports** — replace line 28:

```python
# OLD:
from knowledge_finder_bot.nlm.formatter import format_response

# NEW:
from microsoft_agents.hosting.aiohttp.app.streaming.streaming_response import StreamingResponse
from knowledge_finder_bot.nlm.formatter import format_response, format_source_attribution
```

**3b. Replace lines 204-224** (the nlm-proxy query block) with:

```python
        # --- Streaming nlm-proxy query ---
        streaming = StreamingResponse(context)
        streaming.set_generated_by_ai_label(True)

        conversation_id = session_store.get(aad_object_id) if session_store else None
        notebook_id = None
        new_conversation_id = None
        sent_separator = False

        try:
            async for chunk in nlm_client.query_stream(
                user_message=user_message,
                allowed_notebooks=list(allowed_notebooks),
                conversation_id=conversation_id,
            ):
                if chunk.chunk_type == "meta":
                    if chunk.model and notebook_id is None:
                        notebook_id = chunk.model
                        nb_name = acl_service.get_notebook_name(notebook_id)
                        if nb_name:
                            streaming.queue_informative_update(
                                f"Searching {nb_name}..."
                            )
                    if chunk.conversation_id:
                        new_conversation_id = chunk.conversation_id

                elif chunk.chunk_type == "reasoning":
                    streaming.queue_text_chunk(chunk.text)

                elif chunk.chunk_type == "content":
                    if not sent_separator:
                        streaming.queue_text_chunk("\n\n---\n\n")
                        sent_separator = True
                    streaming.queue_text_chunk(chunk.text)

            source_line = format_source_attribution(notebook_id, acl_service)
            if source_line:
                streaming.queue_text_chunk(source_line)

            await streaming.end_stream()

            if new_conversation_id and session_store:
                session_store.set(aad_object_id, new_conversation_id)

            logger.info(
                "nlm_stream_delivered",
                notebook_id=notebook_id,
                conversation_id=new_conversation_id,
            )

        except Exception as e:
            logger.error("nlm_stream_failed", error=str(e))
            try:
                streaming.queue_text_chunk(
                    "I encountered an error. Please try again."
                )
                await streaming.end_stream()
            except Exception:
                await context.send_activity(
                    "I encountered an error. Please try again."
                )
```

**3c. Remove unused imports** if `Activity` and `ActivityTypes` are no longer used elsewhere. Check: `ActivityTypes.typing` was only used on the deleted line 205. `Activity` is still used on line 205 but that line is now deleted. However, `Activity` and `ActivityTypes` may still be needed elsewhere — check the `on_members_added` handler. `Activity` is not used there (it calls `context.send_activity` with a string). So `Activity` and `ActivityTypes` can be removed from imports. `ConversationUpdateTypes` is still needed.

Update line 20-22 imports:
```python
# OLD:
from microsoft_agents.activity import (
    Activity, ActivityTypes, ConversationUpdateTypes, load_configuration_from_env,
)

# NEW:
from microsoft_agents.activity import (
    ConversationUpdateTypes, load_configuration_from_env,
)
```

**Step 4: Run tests to verify they pass**

Run: `uv run pytest tests/test_bot_nlm.py -v`
Expected: 10 PASSED

**Step 5: Run full suite for regressions**

Run: `uv run pytest tests/ -v`
Expected: All tests pass (72+ existing + ~15 new = ~87 total)

**Step 6: Commit**

```bash
git add src/knowledge_finder_bot/bot/bot.py tests/test_bot_nlm.py
git commit -m "feat(bot): replace buffered nlm query with end-to-end streaming via StreamingResponse"
```

---

### Task 5: Update Documentation

**Files:**
- Modify: `CLAUDE.md` — update architecture diagram and implementation status
- Modify: `.claude/memory/MEMORY.md` — update current phase

**Step 1: Update CLAUDE.md**

In the architecture diagram, the nlm-proxy Client box should note streaming:
```
┌─────── nlm-proxy Client ───────┐
│   POST /v1/chat/completions    │
│   stream=True, SSE chunks      │
│   → StreamingResponse to user  │
└────────────────────────────────┘
```

In Implementation Status, add:
```
**✅ End-to-End Streaming Complete** (branch: `feature/nlm-proxy-integration`)
- NLMClient.query_stream() async generator yields chunks in real-time
- StreamingResponse pipes tokens directly to Teams/DirectLine
- Informative status update with notebook name ("Searching HR Docs...")
- Reasoning + separator + answer content streamed to user
- Source attribution appended at stream end
```

**Step 2: Commit**

```bash
git add CLAUDE.md .claude/memory/MEMORY.md
git commit -m "docs: update architecture and status for end-to-end streaming"
```

---

## Verification Checklist

1. `uv run pytest tests/ -v` — all tests pass
2. `uv run pytest --cov=knowledge_finder_bot tests/` — coverage maintained or improved
3. Manual test with Agent Playground:
   - `uv run python -m knowledge_finder_bot.main`
   - `nport 3978 -s knowledge-finder-bot`
   - Send a question — verify: status update appears, reasoning streams, separator, answer streams, source attribution at end
4. Verify non-streaming fallback: full message arrives as one activity when not on a streaming channel
