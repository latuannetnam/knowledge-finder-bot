# LangChain / LangGraph Agent Migration Analysis

## Current Architecture Snapshot

### What You Have Today

| Component | Technology | Purpose |
|-----------|-----------|---------|
| Query / Streaming | `AsyncOpenAI` (raw SDK) | Calls nlm-proxy Chat Completions endpoint, preserves `reasoning_content` |
| Rewrite / Follow-up | `ChatOpenAI` (LangChain) | Uses `ainvoke()` with message types for LLM tasks |
| Memory | `ConversationMemoryManager` | Custom TTLCache + `langchain-core` message types (`HumanMessage`, `AIMessage`) |
| Orchestration | **Manual** (imperative Python) | Sequential flow in `on_message()`: ACL â†’ rewrite â†’ query â†’ followup â†’ respond |
| Bot Framework | M365 Agents SDK | Handles Teams/DirectLine channels, streaming |

### Installed Versions (as of today)

| Package | Installed | Latest Available |
|---------|-----------|-----------------|
| `langchain-openai` | **1.1.9** | 1.1.9 âœ… (released Feb 10, 2026) |
| `langchain-core` | **1.2.12** | 1.2.x âœ… |
| `langgraph` | **not installed** | 1.0.8 (released Feb 6, 2026) |
| `langchain` | **not installed** | 1.0.x |

### Key Insight

> Your project **already runs the latest LangChain ecosystem packages** (`langchain-openai 1.1.9`, `langchain-core 1.2.12`). You are NOT on outdated versions. The question is whether to adopt **LangGraph** (the orchestration layer) â€” not whether to "upgrade LangChain".

---

## What's New in LangChain / LangGraph 1.0

### LangChain 1.0 (October 2025)

- **`create_agent` abstraction** â€” standard way to build agents, built on LangGraph runtime
- **Standard Content Blocks** â€” provider-agnostic output format across LLMs
- **Streamlined namespace** â€” trimmed down to essential agent components
- **Enhanced Middleware** (v1.1) â€” auto-retries, content moderation, context-aware summarization
- **Deprecated**: `initialize_agent()`, `AgentExecutor` â†’ use `create_agent()` or LangGraph directly

### LangGraph 1.0.8 (February 2026)

- **Graph-based orchestration** â€” model workflows as directed graphs with nodes and edges
- **Durable execution** â€” persist agent state, resume after failures
- **Human-in-the-loop (HITL)** â€” pause for human approval at any node
- **Built-in memory** â€” short-term + long-term persistence across sessions
- **Node-level caching** â€” avoid redundant LLM calls during development
- **Multi-agent systems** â€” orchestrate multiple specialized agents
- **LangSmith integration** â€” tracing, time-travel debugging, evaluation

### Coming: LangGraph 2.0 (Q2 2026)

- Greater API stability
- Improved type safety
- Guardrail nodes (content filtering, rate limiting, compliance)
- Cloud-native persistence backends

---

## Your Bot's Workflow Mapped to LangGraph Concepts

```mermaid
graph TD
    A[User Message] --> B{Is /clear?}
    B -->|Yes| C[Clear Memory]
    B -->|No| D[ACL Check]
    D --> E{Has History?}
    E -->|Yes| F[Rewrite Question]
    E -->|No| G[Query nlm-proxy]
    F --> G
    G --> H{Streaming?}
    H -->|Yes| I[Stream Response]
    H -->|No| J[Buffer Response]
    I --> K[Generate Follow-ups]
    J --> K
    K --> L[Send to User]
```

If you rewrote this as a LangGraph `StateGraph`:

```python
from langgraph.graph import StateGraph, END

builder = StateGraph(ChatState)
builder.add_node("acl_check", acl_check_node)
builder.add_node("rewrite", rewrite_question_node)
builder.add_node("query", query_nlm_proxy_node)
builder.add_node("followup", generate_followup_node)
builder.add_node("respond", send_response_node)

builder.add_edge("acl_check", "rewrite")
builder.add_conditional_edges("rewrite", has_history, {"yes": "query", "no": "query"})
builder.add_edge("query", "followup")
builder.add_edge("followup", "respond")
builder.add_edge("respond", END)

graph = builder.compile(checkpointer=MemorySaver())
```

---

## Should You Migrate? Pros / Cons

### âœ… Pros of Adopting LangGraph

| Benefit | Relevance to Your Bot |
|---------|----------------------|
| **Visual workflow** â€” graph nodes make flow explicit | ğŸŸ¡ Medium â€” your flow is already clear in `on_message()` |
| **Durable state** â€” resume after crashes | ğŸŸ¡ Medium â€” chatbot is stateless per-request, TTLCache is ephemeral by design |
| **Human-in-the-loop** â€” pause for approval | ğŸ”´ Low â€” your bot doesn't need approval steps |
| **Multi-agent** â€” orchestrate multiple LLMs | ğŸ”´ Low â€” you have a single backend (nlm-proxy) |
| **Built-in memory** â€” checkpointed across sessions | ğŸŸ¡ Medium â€” you already have `ConversationMemoryManager` with TTLCache |
| **LangSmith debugging** â€” trace execution | ğŸŸ¢ High â€” useful for production debugging |
| **Future-proof** â€” LangChain ecosystem direction | ğŸŸ¢ High â€” `create_agent` is built on LangGraph |
| **Standardized patterns** â€” community examples | ğŸŸ¡ Medium â€” easier onboarding for new contributors |

### âŒ Cons of Adopting LangGraph

| Concern | Impact |
|---------|--------|
| **Significant refactoring** â€” rewrite `on_message()`, `NLMClient`, memory | ğŸ”´ High â€” ~500+ lines of working, tested code to restructure |
| **New dependency** â€” `langgraph` adds ~50-100MB | ğŸŸ¡ Medium â€” heavier deployment |
| **`reasoning_content` gap** â€” LangChain `ChatOpenAI` still drops `reasoning_content` in Chat Completions SSE format | ğŸ”´ **Critical** â€” your ADR-012 hybrid approach exists because of this. LangGraph won't fix it â€” you'd still need raw `AsyncOpenAI` for streaming |
| **Streaming complexity** â€” M365 SDK `StreamingResponse` is tightly coupled to your bot handler | ğŸ”´ High â€” LangGraph streaming is its own paradigm, integrating with M365 SDK `queue_text_chunk()` is non-trivial |
| **Over-engineering risk** â€” your flow is linear (ACL â†’ rewrite â†’ query â†’ followup â†’ respond) | ğŸŸ¡ Medium â€” LangGraph shines for complex branching, your flow is mostly sequential |
| **Testing overhead** â€” rewrite 90+ tests for new architecture | ğŸ”´ High â€” significant test investment |
| **Learning curve** â€” graph-based thinking, state management, checkpointers | ğŸŸ¡ Medium |
| **LangSmith dependency** â€” full observability requires LangSmith (paid service) | ğŸŸ¡ Medium â€” structlog already provides good logging |

---

## Recommendation

### ğŸŸ¢ Verdict: **Do NOT migrate now. Re-evaluate when triggered.**

Your current architecture is well-designed for its purpose:

1. **Your flow is linear** â€” ACL â†’ rewrite â†’ query â†’ followup â†’ respond. LangGraph's graph-based orchestration is designed for complex branching, loops, and multi-agent collaboration. Your bot doesn't need these.

2. **The `reasoning_content` blocker is still present** â€” LangChain's `ChatOpenAI` (even v1.1.9) does not support `reasoning_content` via Chat Completions SSE. Your ADR-012 hybrid approach (`AsyncOpenAI` for streaming) would still be required, negating a key benefit of going "all LangChain".

3. **M365 SDK coupling** â€” Your `StreamingResponse` integration with `queue_text_chunk()` / `queue_informative_update()` is specific to the M365 Agents SDK. LangGraph's streaming model is different and would require a complex adapter layer.

4. **90+ tests passing** â€” The current codebase is stable and well-tested. A LangGraph migration would require rewriting most tests for minimal functional gain.

5. **You're on the latest packages** â€” `langchain-openai 1.1.9` and `langchain-core 1.2.12` are current. You're not missing features.

### â° Migration Triggers (re-evaluate when any of these arise)

| Trigger | Why It Matters |
|---------|---------------|
| **Multi-backend routing** â€” need to query multiple LLMs/services per request | LangGraph's conditional edges and parallel nodes are ideal |
| **Multi-step reasoning** â€” need tool use, web search, code execution within the bot | LangGraph's agent loop with tool calling is the standard |
| **Approval workflows** â€” need human-in-the-loop for sensitive queries | LangGraph HITL is first-class |
| **LangChain fixes `reasoning_content`** â€” ChatOpenAI supports `reasoning_content` via Chat Completions | Then you can drop `AsyncOpenAI` entirely and go full LangChain/LangGraph |
| **LangGraph 2.0** â€” expected Q2 2026 with better API stability | Cleaner migration target |

### ğŸ› ï¸ Low-Cost Improvements You Can Do Now

Instead of a full migration, consider these incremental improvements that use newer LangChain features:

1. **Upgrade `pyproject.toml` constraints** â€” pin `langchain-openai>=1.1.0` and `langchain-core>=1.2.0` to formalize that you're on v1.x

2. **Monitor `reasoning_content` support** â€” track [langchain-ai/langchain#reasoning-content](https://github.com/langchain-ai/langchain/issues) for Chat Completions SSE support. When fixed, you can unify `NLMClient` to use only `ChatOpenAI`.

3. **Consider LangSmith for observability** â€” even without LangGraph, you can enable `LANGCHAIN_TRACING_V2=true` to get traces from your existing `ChatOpenAI.ainvoke()` calls

---

## Summary Table

| Aspect | Current (Hybrid) | LangGraph Migration |
|--------|-------------------|---------------------|
| **Complexity** | âœ… Simple, linear | âš ï¸ Graph overhead for simple flow |
| **Streaming** | âœ… Native SSE via AsyncOpenAI | âš ï¸ Requires adapter for M365 SDK |
| **reasoning_content** | âœ… Works via raw SDK | âŒ Still broken in ChatOpenAI |
| **Memory** | âœ… TTLCache, sufficient | ğŸŸ¡ LangGraph has better built-in, but overkill |
| **Testing** | âœ… 90+ tests passing | âŒ Major rewrite needed |
| **Dependencies** | âœ… Lightweight | âš ï¸ +langgraph (~100MB) |
| **Future-proof** | ğŸŸ¡ Good, on latest LangChain | âœ… On the official path |
| **Multi-agent** | âŒ Not supported | âœ… First-class |
| **HITL** | âŒ Not supported | âœ… First-class |
